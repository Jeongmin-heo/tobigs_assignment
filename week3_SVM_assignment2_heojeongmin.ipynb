{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "## 1. Assignment2는 제가 anomaly-detection 데이터셋(캐글에 올라와있음)을 드립니다. \n",
    "이것은 해당 결제가 사기인지 아닌 지 판별하는 데이터셋이며, 실습코드를 활용하고, 또 본인이 여태 배운 내용을 활용하여 자유롭게 데\n",
    "이터를 가지고 연습해주시면 됩니다. 정말 시간이 없으시면, 실습코드를 조금만 바꿔서 모델을 트레이닝하고 평\n",
    "가한 결과만 보여주셔도 과제를 돌려보내지는 않겠습니다.\n",
    "## 2. 다만 이 데이터셋은 굉장히 imbalance한 데이터 셋입니다. \n",
    "실제 사기를 치는 사례가 많지 않으므로 사기인 경우가 전체에 0.17프로밖에 되지 않습니다. 따라서, 그냥 데이터를 트레이닝 시키면 무조건 사기가 아니라고 판별해\n",
    "버릴 가능성이 높습니다. 또한, 그대로 트레이닝을 돌리게 되면 엄청나게 많은 데이터 양 때문에 트레이닝조차\n",
    "힘들 것입니다. 그런데 실제로 정확도를 높이면서 트레이닝을 할 수 있는 방법이 있으니 고민해보세요\n",
    "(아마 이미 데이터 분석을 해보신분들은 imbalance라는 단어를 보자마자 답을 떠올리셨을테지만, 처음 해보시는 분\n",
    "들은 고민을 해보신 뒤, 멘토분들 아무나 붙잡고 물어보시면 답을 말해줄 것입니다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "detection = pd.read_csv('creditcard.csv',encoding='utf-8')\n",
    "#데이터를 불러왔습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 284806 entries, 0 to 284805\n",
      "Data columns (total 31 columns):\n",
      "Time      284806 non-null float64\n",
      "V1        284806 non-null float64\n",
      "V2        284806 non-null float64\n",
      "V3        284806 non-null float64\n",
      "V4        284806 non-null float64\n",
      "V5        284806 non-null float64\n",
      "V6        284806 non-null float64\n",
      "V7        284806 non-null float64\n",
      "V8        284806 non-null float64\n",
      "V9        284806 non-null float64\n",
      "V10       284806 non-null float64\n",
      "V11       284806 non-null float64\n",
      "V12       284806 non-null float64\n",
      "V13       284806 non-null float64\n",
      "V14       284806 non-null float64\n",
      "V15       284806 non-null float64\n",
      "V16       284806 non-null float64\n",
      "V17       284806 non-null float64\n",
      "V18       284806 non-null float64\n",
      "V19       284806 non-null float64\n",
      "V20       284806 non-null float64\n",
      "V21       284806 non-null float64\n",
      "V22       284806 non-null float64\n",
      "V23       284806 non-null float64\n",
      "V24       284806 non-null float64\n",
      "V25       284806 non-null float64\n",
      "V26       284806 non-null float64\n",
      "V27       284806 non-null float64\n",
      "V28       284806 non-null float64\n",
      "Amount    284806 non-null float64\n",
      "Class     284806 non-null int64\n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 67.4 MB\n"
     ]
    }
   ],
   "source": [
    "detection.info()\n",
    "#데이터는 28만여개로 보입니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    284314\n",
       "1       492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.value_counts(detection['Class']).plot.bar()\n",
    "plt.title('Fraud class histogram')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Frequency')\n",
    "detection['Class'].value_counts()\n",
    "#class를 확인해봤을때 0에 비하여 1인경우가 매우 적은 것으로 확인되었습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "      <th>normAmount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0</td>\n",
       "      <td>0.244966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.342472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>0</td>\n",
       "      <td>1.160686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0</td>\n",
       "      <td>0.140536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.073401</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9       V10  ...       V21       V22       V23       V24  \\\n",
       "0  0.098698  0.363787  0.090794  ... -0.018307  0.277838 -0.110474  0.066928   \n",
       "1  0.085102 -0.255425 -0.166974  ... -0.225775 -0.638672  0.101288 -0.339846   \n",
       "2  0.247676 -1.514654  0.207643  ...  0.247998  0.771679  0.909412 -0.689281   \n",
       "3  0.377436 -1.387024 -0.054952  ... -0.108300  0.005274 -0.190321 -1.175575   \n",
       "4 -0.270533  0.817739  0.753074  ... -0.009431  0.798278 -0.137458  0.141267   \n",
       "\n",
       "        V25       V26       V27       V28  Class  normAmount  \n",
       "0  0.128539 -0.189115  0.133558 -0.021053      0    0.244966  \n",
       "1  0.167170  0.125895 -0.008983  0.014724      0   -0.342472  \n",
       "2 -0.327642 -0.139097 -0.055353 -0.059752      0    1.160686  \n",
       "3  0.647376 -0.221929  0.062723  0.061458      0    0.140536  \n",
       "4 -0.206010  0.502292  0.219422  0.215153      0   -0.073401  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detection['normAmount'] = StandardScaler().fit_transform(detection['Amount'].values.reshape(-1, 1))\n",
    "detection = detection.drop(['Time', 'Amount'], axis=1)\n",
    "detection.head()\n",
    "#샘플링을 하기전에 모든 데이터에 대해서 스케일링을 진행하였습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "      <th>normAmount</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td rowspan=\"5\" valign=\"top\">0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0</td>\n",
       "      <td>0.244966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>0</td>\n",
       "      <td>1.160686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0</td>\n",
       "      <td>0.140536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>-0.894286</td>\n",
       "      <td>0.286157</td>\n",
       "      <td>-0.113192</td>\n",
       "      <td>-0.271526</td>\n",
       "      <td>2.669599</td>\n",
       "      <td>3.721818</td>\n",
       "      <td>0.370145</td>\n",
       "      <td>0.851084</td>\n",
       "      <td>-0.392048</td>\n",
       "      <td>-0.410430</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.073425</td>\n",
       "      <td>-0.268092</td>\n",
       "      <td>-0.204233</td>\n",
       "      <td>1.011592</td>\n",
       "      <td>0.373205</td>\n",
       "      <td>-0.384157</td>\n",
       "      <td>0.011747</td>\n",
       "      <td>0.142404</td>\n",
       "      <td>0</td>\n",
       "      <td>0.019394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>-0.338262</td>\n",
       "      <td>1.119593</td>\n",
       "      <td>1.044367</td>\n",
       "      <td>-0.222187</td>\n",
       "      <td>0.499361</td>\n",
       "      <td>-0.246761</td>\n",
       "      <td>0.651583</td>\n",
       "      <td>0.069539</td>\n",
       "      <td>-0.736727</td>\n",
       "      <td>-0.366846</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.246914</td>\n",
       "      <td>-0.633753</td>\n",
       "      <td>-0.120794</td>\n",
       "      <td>-0.385050</td>\n",
       "      <td>-0.069733</td>\n",
       "      <td>0.094199</td>\n",
       "      <td>0.246219</td>\n",
       "      <td>0.083076</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.338514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td rowspan=\"5\" valign=\"top\">1</td>\n",
       "      <td>275992</td>\n",
       "      <td>-2.027135</td>\n",
       "      <td>-1.131890</td>\n",
       "      <td>-1.135194</td>\n",
       "      <td>1.086963</td>\n",
       "      <td>-0.010547</td>\n",
       "      <td>0.423797</td>\n",
       "      <td>3.790880</td>\n",
       "      <td>-1.155595</td>\n",
       "      <td>-0.063434</td>\n",
       "      <td>1.334414</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.315105</td>\n",
       "      <td>0.575520</td>\n",
       "      <td>0.490842</td>\n",
       "      <td>0.756502</td>\n",
       "      <td>-0.142685</td>\n",
       "      <td>-0.602777</td>\n",
       "      <td>0.508712</td>\n",
       "      <td>-0.091646</td>\n",
       "      <td>1</td>\n",
       "      <td>2.182756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>276864</td>\n",
       "      <td>-1.374424</td>\n",
       "      <td>2.793185</td>\n",
       "      <td>-4.346572</td>\n",
       "      <td>2.400731</td>\n",
       "      <td>-1.688433</td>\n",
       "      <td>0.111136</td>\n",
       "      <td>-0.922038</td>\n",
       "      <td>-2.149930</td>\n",
       "      <td>-2.027474</td>\n",
       "      <td>-4.390842</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.870779</td>\n",
       "      <td>0.504849</td>\n",
       "      <td>0.137994</td>\n",
       "      <td>0.368275</td>\n",
       "      <td>0.103137</td>\n",
       "      <td>-0.414209</td>\n",
       "      <td>0.454982</td>\n",
       "      <td>0.096711</td>\n",
       "      <td>1</td>\n",
       "      <td>1.042423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>279863</td>\n",
       "      <td>-1.927883</td>\n",
       "      <td>1.125653</td>\n",
       "      <td>-4.518331</td>\n",
       "      <td>1.749293</td>\n",
       "      <td>-1.566487</td>\n",
       "      <td>-2.010494</td>\n",
       "      <td>-0.882850</td>\n",
       "      <td>0.697211</td>\n",
       "      <td>-2.064945</td>\n",
       "      <td>-5.587794</td>\n",
       "      <td>...</td>\n",
       "      <td>0.778584</td>\n",
       "      <td>-0.319189</td>\n",
       "      <td>0.639419</td>\n",
       "      <td>-0.294885</td>\n",
       "      <td>0.537503</td>\n",
       "      <td>0.788395</td>\n",
       "      <td>0.292680</td>\n",
       "      <td>0.147968</td>\n",
       "      <td>1</td>\n",
       "      <td>1.206024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280143</td>\n",
       "      <td>1.378559</td>\n",
       "      <td>1.289381</td>\n",
       "      <td>-5.004247</td>\n",
       "      <td>1.411850</td>\n",
       "      <td>0.442581</td>\n",
       "      <td>-1.326536</td>\n",
       "      <td>-1.413170</td>\n",
       "      <td>0.248525</td>\n",
       "      <td>-1.127396</td>\n",
       "      <td>-3.232153</td>\n",
       "      <td>...</td>\n",
       "      <td>0.370612</td>\n",
       "      <td>0.028234</td>\n",
       "      <td>-0.145640</td>\n",
       "      <td>-0.081049</td>\n",
       "      <td>0.521875</td>\n",
       "      <td>0.739467</td>\n",
       "      <td>0.389152</td>\n",
       "      <td>0.186637</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.350189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>281144</td>\n",
       "      <td>-3.113832</td>\n",
       "      <td>0.585864</td>\n",
       "      <td>-5.399730</td>\n",
       "      <td>1.817092</td>\n",
       "      <td>-0.840618</td>\n",
       "      <td>-2.943548</td>\n",
       "      <td>-2.208002</td>\n",
       "      <td>1.058733</td>\n",
       "      <td>-1.632333</td>\n",
       "      <td>-5.245984</td>\n",
       "      <td>...</td>\n",
       "      <td>0.583276</td>\n",
       "      <td>-0.269209</td>\n",
       "      <td>-0.456108</td>\n",
       "      <td>-0.183659</td>\n",
       "      <td>-0.328168</td>\n",
       "      <td>0.606116</td>\n",
       "      <td>0.884876</td>\n",
       "      <td>-0.253700</td>\n",
       "      <td>1</td>\n",
       "      <td>0.626303</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>142403 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    V1        V2        V3        V4        V5        V6  \\\n",
       "Class                                                                      \n",
       "0     0      -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388   \n",
       "      2      -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499   \n",
       "      3      -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203   \n",
       "      8      -0.894286  0.286157 -0.113192 -0.271526  2.669599  3.721818   \n",
       "      9      -0.338262  1.119593  1.044367 -0.222187  0.499361 -0.246761   \n",
       "...                ...       ...       ...       ...       ...       ...   \n",
       "1     275992 -2.027135 -1.131890 -1.135194  1.086963 -0.010547  0.423797   \n",
       "      276864 -1.374424  2.793185 -4.346572  2.400731 -1.688433  0.111136   \n",
       "      279863 -1.927883  1.125653 -4.518331  1.749293 -1.566487 -2.010494   \n",
       "      280143  1.378559  1.289381 -5.004247  1.411850  0.442581 -1.326536   \n",
       "      281144 -3.113832  0.585864 -5.399730  1.817092 -0.840618 -2.943548   \n",
       "\n",
       "                    V7        V8        V9       V10  ...       V21       V22  \\\n",
       "Class                                                 ...                       \n",
       "0     0       0.239599  0.098698  0.363787  0.090794  ... -0.018307  0.277838   \n",
       "      2       0.791461  0.247676 -1.514654  0.207643  ...  0.247998  0.771679   \n",
       "      3       0.237609  0.377436 -1.387024 -0.054952  ... -0.108300  0.005274   \n",
       "      8       0.370145  0.851084 -0.392048 -0.410430  ... -0.073425 -0.268092   \n",
       "      9       0.651583  0.069539 -0.736727 -0.366846  ... -0.246914 -0.633753   \n",
       "...                ...       ...       ...       ...  ...       ...       ...   \n",
       "1     275992  3.790880 -1.155595 -0.063434  1.334414  ... -0.315105  0.575520   \n",
       "      276864 -0.922038 -2.149930 -2.027474 -4.390842  ... -0.870779  0.504849   \n",
       "      279863 -0.882850  0.697211 -2.064945 -5.587794  ...  0.778584 -0.319189   \n",
       "      280143 -1.413170  0.248525 -1.127396 -3.232153  ...  0.370612  0.028234   \n",
       "      281144 -2.208002  1.058733 -1.632333 -5.245984  ...  0.583276 -0.269209   \n",
       "\n",
       "                   V23       V24       V25       V26       V27       V28  \\\n",
       "Class                                                                      \n",
       "0     0      -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053   \n",
       "      2       0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752   \n",
       "      3      -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458   \n",
       "      8      -0.204233  1.011592  0.373205 -0.384157  0.011747  0.142404   \n",
       "      9      -0.120794 -0.385050 -0.069733  0.094199  0.246219  0.083076   \n",
       "...                ...       ...       ...       ...       ...       ...   \n",
       "1     275992  0.490842  0.756502 -0.142685 -0.602777  0.508712 -0.091646   \n",
       "      276864  0.137994  0.368275  0.103137 -0.414209  0.454982  0.096711   \n",
       "      279863  0.639419 -0.294885  0.537503  0.788395  0.292680  0.147968   \n",
       "      280143 -0.145640 -0.081049  0.521875  0.739467  0.389152  0.186637   \n",
       "      281144 -0.456108 -0.183659 -0.328168  0.606116  0.884876 -0.253700   \n",
       "\n",
       "              Class  normAmount  \n",
       "Class                            \n",
       "0     0           0    0.244966  \n",
       "      2           0    1.160686  \n",
       "      3           0    0.140536  \n",
       "      8           0    0.019394  \n",
       "      9           0   -0.338514  \n",
       "...             ...         ...  \n",
       "1     275992      1    2.182756  \n",
       "      276864      1    1.042423  \n",
       "      279863      1    1.206024  \n",
       "      280143      1   -0.350189  \n",
       "      281144      1    0.626303  \n",
       "\n",
       "[142403 rows x 30 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sampling_func(data, sample_pct):\n",
    "\n",
    "    np.random.seed(123)\n",
    "\n",
    "    N = len(data)\n",
    "\n",
    "    sample_n = int(len(data)*sample_pct) # integer\n",
    "\n",
    "    sample = data.take(np.random.permutation(N)[:sample_n])\n",
    "\n",
    "    return sample\n",
    "\n",
    "detection = detection.groupby('Class').apply(sampling_func, sample_pct=0.5)\n",
    "detection.sort_index()\n",
    "#데이터가 제 노트북에서 SVM을 돌리기에는 너무 많기 때문에 랜덤추출을 진행하였습니다. 이러한 방법은 데이터의 왜곡을 부를수 있을 것 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (142403, 29)\n",
      "Shape of y: (142403, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gjwjd\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#ix-indexer-is-deprecated\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\gjwjd\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:993: FutureWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#ix-indexer-is-deprecated\n",
      "  obj = getattr(obj, self.name)._getitem_axis(key, axis=axis)\n",
      "C:\\Users\\gjwjd\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#ix-indexer-is-deprecated\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "X = np.array(detection.ix[:, detection.columns != 'Class'])\n",
    "y = np.array(detection.ix[:, detection.columns == 'Class'])\n",
    "print('Shape of X: {}'.format(X.shape))\n",
    "print('Shape of y: {}'.format(y.shape))\n",
    "#detection을 X와 y로 나누고 그 형태에 대해서 확인하였습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train dataset 갯수:  (99682, 29)\n",
      "y_train dataset 갯수:  (99682, 1)\n",
      "X_test dataset 갯수:  (42721, 29)\n",
      "y_test dataset 갯수:  (42721, 1)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "print(\"X_train dataset 갯수: \", X_train.shape)\n",
    "print(\"y_train dataset 갯수: \", y_train.shape)\n",
    "print(\"X_test dataset 갯수: \", X_test.shape)\n",
    "print(\"y_test dataset 갯수: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이러한 불균형한 데이터에 대해서 SMOTE라는  OverSampling을 진행하였습니다. \n",
    "진행전과 후에 불균형 했던 class가 동일한 갯수를 이루는 것을 알 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before OverSampling, counts of label '1': [164]\n",
      "Before OverSampling, counts of label '0': [99518] \n",
      "\n",
      "After OverSampling, the shape of train_X: (199036, 29)\n",
      "After OverSampling, the shape of train_y: (199036,) \n",
      "\n",
      "After OverSampling, counts of label '1': 99518\n",
      "After OverSampling, counts of label '0': 99518\n"
     ]
    }
   ],
   "source": [
    "print(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train==1)))\n",
    "print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train==0)))\n",
    "\n",
    "sm = SMOTE(random_state=2)\n",
    "X_train_res, y_train_res = sm.fit_sample(X_train, y_train.ravel())\n",
    "\n",
    "print('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape))\n",
    "print('After OverSampling, the shape of train_y: {} \\n'.format(y_train_res.shape))\n",
    "\n",
    "print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res==1)))\n",
    "print(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res==0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 linear SVM을 적용하였습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import rbf_kernel \n",
    "svc=SVC(kernel='linear', C = 10) \n",
    "svc.fit(X_train_res,y_train_res) # 모델 트레이닝 하는 코드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "트레이닝 후에 훈련한 모델에 대해서 test set을 가지고 확인해 봤을 때 매우 높은 점수를 나타내는 것을 확인할 수 있었습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "y_pred=svc.predict(X_test) # 훈련한 모델로 test셋을 시험해보자\n",
    "print('Accuracy Score:') \n",
    "metrics.accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또한 과제1과 같이 rbf커널로 트레이닝을 진행했을때도 매우 높은 정확도를 나타내는 것을 알 수 있었습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_rbf=SVC(kernel='rbf', gamma = 1) #rbf커널로해보자\n",
    "svc_rbf.fit(X_train_res,y_train_res) # 모델트리이닝\n",
    "y_pred=svc_rbf.predict(X_test)\n",
    "print('Accuracy Score:')\n",
    "\n",
    "print(metrics.accuracy_score(y_test,y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
