{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment1 ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 앙상블\n",
    "- 여러 모델을 사용하여 예측 정확도를 높이는데 사용\n",
    "- 즉, 여러개의 약분류기(Weak Classifier)를 합하여 하나의 강분류기를 만드는데 목적\n",
    "- 예측 변수 그룹(예: 의사결정 트리 분류기, SVM, 로지스틱 회귀)의 예측을 집계하면, 최상의 개별 예측 변수보다 더 나은 예측을 얻을 수 있는 경우가 많다.\n",
    "\n",
    "### 1.1 기대효과\n",
    "- 정확성이 향상된다.\n",
    "- 과적합을 방지할 수 있다.\n",
    "- 개별 모형보다 좋은 성능을 가진다\n",
    "\n",
    "## 2 앙상블의 유형\n",
    "1. Basic Ensemble Techniques\n",
    "2. Advanced Ensemble Techniques\n",
    "\n",
    "### 2.1 Basic Ensemble Techniques\n",
    " \n",
    "#### 2.1.1 Max Voting, Averaging, Weighted Average\n",
    "- classification problems문제에 일반적으로 사용\n",
    "\n",
    "#### 2.1.2 Majority Voting\n",
    "- 가장 단순한 모형 결합 방법으로 전혀 다른 모형도 결합가능\n",
    "- hard, soft 두가지의 방식으로 나눌 수 있음\n",
    "- hard voting : 단순합 투표, 성능에만 관심\n",
    "- soft voting : 가중치 투표, 각 분류기의 예측을 평균내어 가장 확률이 높은 클래스를 선택\n",
    "\n",
    "### 2.2 Advanced Ensemble Techniques\n",
    "\n",
    "#### 2.2.1 Bagging\n",
    "- Gagging은 샘플을 여러번 뽑아서(Bootstrap) 각 모델을 학습시켜 결과물을 집계하는 방법\n",
    "- 동일한 모델을 데이터만 분할하여 여러개 모델을 학습\n",
    "- 배깅 = 부트스트랩 + 다수결\n",
    "- OOB 평가 = Bagging 과정에서 sampling 되지 않은 비율 OOB는 Validation 효과를 줄 수 있음\n",
    "- Baigging의 예: Bagging meta-estimator, Random Forest(DT의 합 -> RF) 등\n",
    "\n",
    "##### 2.2.1.1 RandomForest의 파라미터\n",
    "- n_estimators: 임의의 숲에서 생성될 의사결정 나무의 수를 정의한다. 일반적으로 숫자가 높을수록 예측은 더 강해지고 안정적이지만, 매우 큰 숫자는 더 많은 훈련 시간을 야기할 수 있다.\n",
    "- criterion: 분할에 사용할 기능을 정의한다. \n",
    "- max_features : 각 의사결정 트리의 분할에 허용되는 최대 fetatures의 수를 정의한다.\n",
    "- max_deep: 트리의 최대 깊이를 정의한다. \n",
    "- min_samples_split: 분할을 시도하기 전에 필요한 최소 샘플 수를 정의하는 데 사용된다. 샘플 수가 필요한 수보다 적으면 노드는 분할되지 않는다.\n",
    "- min_samples_leaf: 이는 리프 노드에 있어야 하는 최소 샘플 수를 정의한다. 잎 크기가 작을수록 모델은 열차 데이터에서 소음을 포착하기 쉽다.\n",
    "- max_leaf_node: 이 매개 변수는 각 트리의 최대 리프 노드 수를 지정한다. 잎 노드의 수가 최대 리프 노드와 같아지면 트리는 분할을 중지한다.\n",
    "- n_jobs: 병렬로 실행할 작업 수를 나타낸다\n",
    "\n",
    "\n",
    "#### 2.2.2 Boosting\n",
    "- 가중치를 활용하여 약 분류기를 강분류기로 만드는 방법\n",
    "- 부스팅은 배깅(취합방법론)과 다르게 각 모델의 가중치를 반영한다\n",
    "- 부스팅의 예:AdaBoost(depth=1 이진분류,과소적합됐던 데이터 샘플의 가중치를 높히는 방식으로 반복)\n",
    "- AdaBoost의 단점: weight가 낮은 data주변에 weight가 높은 data가 존재한다면 잘못 분류하게 되면서 성능이 떨어질 가능성\n",
    "- 이를 극복한 방법이 Gradient Descent 알고리즘을 활용하여 MSE를 최소화하는 Gradient Boosting!\n",
    "- Gradient Boosting: 학습 전단계 모델에서의 잔여오차에 대해서 새로운 모델을 학습시키는 방법\n",
    "- Gradient Boosting을 기반으로 발전한 모델: XGB, Light GBM\n",
    "- XGB: 과적합을 막기위한 정규화 모델을 사용, 성능은 좋지만 학습시간이 오래걸리는 단점\n",
    "- Light GBM : 한쪽으로만 성장하기 때문에 학습시간이 훨씬 적음 다만, 데이터의 수가 적으면 과적합의 가능성\n",
    "\n",
    "##### 2.2.2.1 AdaBoost의 파라미터\n",
    "- base_estimators: base extimator의 유형\n",
    "- n_estimators: estimators의 수를 정의한다. 기본값은 10이지만 더 높은 값을 유지해야 더 나은 성능을 얻을 수 있다.\n",
    "- Learning_rate: 이 매개변수는 최종 조합에서 추정자의 기여도를 관리한다. \n",
    "- max_deep: 최대 깊이를 정한다. \n",
    "- n_jobs: 사용할 수 있는 프로세서 수를 \n",
    "- random_state : 랜덤 데이터 분할을 지정하는 정수 값. random_state의 확실한 값은 매개변수와 훈련 데이터가 같을 경우 항상 동일한 결과를 산출한다.\n",
    "\n",
    "\n",
    "### 2.3 Stacking\n",
    "- 서로 다른 모델들을 조합하여 최고의 성능을 내는 모델을 생성\n",
    "- 예측값을 새로운 학습데이터로 사용하여 최종모델에 반영하는 방식\n",
    "\n",
    "## 3. Error\n",
    "앙상블 모델에서 무엇이 오류를 일으키는지를 이해할 필요가 있다. 이 점에서 각 앙상블 학습자에게 통찰력을 줄 것이다.\n",
    "\n",
    "### 3.1 Bias error\n",
    "\n",
    "Bias error는 평균적으로 예측값이 실제 값과 다른 정도를 정량화하는 데 유용하다. 높은 바이어스 오류는 중요한 추세를 계속 놓치는 저성능 모델을 가지고 있다는 것을 의미한다.\n",
    "\n",
    "### 3.2 Variance\n",
    "\n",
    "Variance는 동일한 관측에 대한 예측이 서로 어떻게 다른지 수량화한다. 높은 분산 모델은 훈련모델의 과적합을 만들수도있다.\n",
    "\n",
    "또한 모델은 두 종류의 error사이에서 balance를 유지해야한다.\n",
    "<img src=\"https://i.imgur.com/jFfarvo.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
